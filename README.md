# Multi-Source-ETL-Pipeline-with-Airflow-DAG-Architecture

## Project Overview
This project implements a comprehensive ETL (Extract, Transform, Load) pipeline that processes data from multiple sources, transforms it according to business rules, and loads it into a dimensional data warehouse for analysis.

## Data Sources
- **Customer Data**: Brazilian E-commerce dataset (CSV)
- **Product Data**: Amazon product dataset (JSON)
- **Order Data**: UK Online Retail dataset (SQLite)
- **Web Traffic**: Synthetic web server logs (CSV)
- > ğŸ“ Note: All datasets are fictional and sanitized â€” no real customer or business data is included.

## Features
- Extract data from multiple sources with different formats
- Transform data with data quality checks and error handling
- Load data into a dimensional data warehouse model
- Orchestrate the pipeline with Airflow DAG architecture
- Handle errors and data quality issues gracefully

## Technical Architecture
- **Languages**: Python
- **Data Handling**: Pandas, SQLite
- **File Formats**: CSV, JSON, SQL
- **Workflow**: DAG-based task orchestration
- **Error Handling**: Comprehensive exception management

## Project Structure
ecommerce_etl/
â”œâ”€â”€ dags/                      # DAG definitions
â”‚   â””â”€â”€ ecommerce_pipeline.py  # Airflow DAG definition
â”œâ”€â”€ data/                      # Data sources
â”‚   â”œâ”€â”€ customers/             # Customer data files
â”‚   â”œâ”€â”€ products/              # Product data files
â”‚   â”œâ”€â”€ orders/                # Order data files
â”‚   â”œâ”€â”€ logs/                  # Web log files
â”‚   â””â”€â”€ ecommerce.db           # SQLite database
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ prepare_product_data.py  # Prepare product JSON
â”‚   â”œâ”€â”€ generate_web_logs.py     # Generate web logs
â”‚   â””â”€â”€ setup_database.py        # Set up order database
â”œâ”€â”€ airflow_etl_simulation.py  # ETL pipeline with Airflow simulation
â”œâ”€â”€ requirements.txt           # Project dependencies
â””â”€â”€ README.md                  # Project documentation

## Running the Pipeline

1. Install required dependencies:
   pip install -r requirements.txt

2. Set up the data directories and prepare data:
   python scripts/prepare_product_data.py
   python scripts/generate_web_logs.py
   python scripts/setup_database.py

3. Run the ETL pipeline:
   python airflow_etl_simulation.py

4. Verify the results in the SQLite database (`data/ecommerce.db`):
- `dim_customers`: Customer dimension
- `dim_products`: Product dimension
- `fact_orders`: Order transactions
- `fact_web_traffic`: Web traffic data

## Data Transformations
- Customer data: Cleaning, standardization, NULL handling
- Product data: Type conversions, category standardization
- Order data: Negative amount correction, aggregation
- Web logs: Timestamp parsing, structure normalization

## Results
The pipeline processes over 130,000 records from four distinct data sources:
- 99,441 customer records
- 1,341 product records
- 18,536 order records
- 10,000 web traffic records

## Technical Challenges Addressed
- Schema evolution and mismatches
- Data type conversions
- Primary key handling (string vs. integer)
- Missing value imputation
- Error recovery
- Task dependency management
